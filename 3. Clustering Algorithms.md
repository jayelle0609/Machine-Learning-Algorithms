## UNSUPERVISED LEARNING

In the previous posts, we’ve discussed Supervised Learning (Regression & Classification) - learning from “labelled” data. The data provided inputs (x, features) and outputs (y, target) and allowed us to come up with models to predict future outcomes, based on characteristics given (inputs). 

But in this post it will be different. Unsupervised Learning involves data with inputs (features) only. There are no correct answers or labels given (target). In other words, there’s only input data but there’s no output. There’s no supervision when learning from data.

In fact, unsupervised learning is algorithms trying to discover new insights from data. The goal is to find patterns, structures or "clusters" in the data.

- Clustering
- Dimensionality Reduction
- Anomaly Detection 

We’re not trying to predict an outcome here. The goal is to “divide” the dataset into groups with some similarities. For example, each ecommerce customer might belong to a particular group (e.g. given their income and spending level). If we have gathered enough data points, it’s likely there are aggregates.

At first the data points will seem scattered (no pattern at all). But once we apply a clustering algorithm, the data will somehow make sense because we’ll be able to easily visualize the groups or clusters. This will also reveal outliers for Anomaly Detection.

When it comes to determining the number of clusters and which data point should belong to a certain cluster, it’s up to our objective and the context we are looking into, to determine the number of clusters, and how we want to group the clusters in big data. There's no rules set in stone, it really depends on what your purpose is and what you want to find out from your data.

## K-Means Clustering

- Partioning objects into k clusters, based on feature similarity (similar characteristics) to derive insights on relationships between features.
- Ex. Grouping customers according to Annual Income and Spending Score.

Now, we will look at data about Mall Customers, where info about their Gender, Age, Annual Income and Spending Score are indicated. (The higher the spending score, the more they spend at the mall.)

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
%matplotlib inline

dataset = pd.read_csv('Mall_Customers.csv') 
dataset.head(10)
```
<img width="474" height="321" alt="image" src="https://github.com/user-attachments/assets/e95303a9-001c-4a78-913a-dce6fd3b563e" />

### Group customers into 5 distinct groups:
1. Medium Annual Income (AI), Medium Spending Score (SS)
2. High AI, Low SS
3. High AI, High SS
4. Low AI, Low SS
5. Low AI, High SS

It will helpful to pay attention to Cluster #2. If a sizable amount of customers fall in Cluster #2, these customers have high AI, but are spending or using most of their money elsewhere (not in the mall). This would be really helpful information for the mall, it could be a huge opportunity for the mall, for example, the marketing dept could formulate specific strategies to entice Cluster #2 to buy more from the mall.

### Elbow Method and Within Cluster Sum of Squares (WCSS) to find optimal no. of k clusters
To find the optimal number of k clusters, the Elbow Method and WCSS (within-cluster sums of squares) can be used. Here’s how we can code to accomplish this:

```python
From sklearn.cluster import KMeans
wcss = [ ]
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()


kmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)
y_kmeans = kmeans.fit_predict(X) 
plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')
plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')
plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')
plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')
plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')

plt.title('Clusters of customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()
```
<img width="411" height="278" alt="image" src="https://github.com/user-attachments/assets/b752de08-ec01-455f-a3a7-0630221f0d0f" />

Notice that the “elbow” points at 5 (number of clusters). Coincidentally, this number was also the “desired” number of groups that will subdivide the dataset according to their Annual Income and Spending Score.
After determining the optimal number of clusters, we can then proceed with applying K-Means to the dataset and then performing data visualization:

```python
kmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)
y_kmeans = kmeans.fit_predict(X)

plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')
plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')
plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')
plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 100, c = 'cyan',label = 'Cluster 4')
plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')

plt.title('Clusters of customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()
```
<img width="392" height="278" alt="image" src="https://github.com/user-attachments/assets/b209a9fd-6a2f-405b-879d-f9c99e186d52" />

### Insights
There we have it. We have 5 clusters and cluster #2 (High Annual Income and Low Spending Score) size seems significant enough. It might be worthwhile for the marketing department to focus on that group.
Also notice the Centroids (the yellow points). This is a part of how K-Means clustering works. It’s an iterative approach where random points are placed initially until they converge to a minimum (e.g. sum of distances is minimized).

As mentioned earlier, it can all be arbitrary and it may depend heavily on our judgment and possible application. We can set n_clusters into anything other than 5. We only used the Elbow Method so we can have a more sound and consistent basis for the number of clusters. But it’s still up to our judgment what
should we use and if the results are good enough for our application.

## Anomaly Detection
Clustering reveals the natural clusters, and also obvious anomalies that don't belong to the clusters. It helps us detect outliers in our data and large deviations. 

Any large deviation from the normal, with respect to the SD and mean, could ring our alarm bells. 

Ex. Is a credit card transaction fraudulent? Is a login activity suspicious? there’s an anomaly, how
serious are the consequences?

# Association Rule Learning 
- Apriori
- A continuation of unsupervised learning.
- The goal is to discover how features are "related" or associated with one another. 

### Application in real life 
- This can be very useful in determining which products should be placed together in grocery
stores.
- For instance, many customers might always be buying bread and milk together.
- Customers could be buying diapers online and then later on, books on parenting.
- In certain life stages of a customer, certain products will have strong associations with each other.
- A good way to recommend related products to customers, forecast future demands and optimize sales.

  ### How to determine the level of "relatedness" or strength of association?
  - No set way to do so. Some suggestions below.
  - Calculate percentages of purchases of A and B, out of total. We could set a arbitrary threshold (ex. 30%)
  - Probability of A being bought with B?
  - Priotise sales of items that have higher probabilities (or popularities) of being bought together, to encourage more transactions.
  - Test different combinations of products, to see which yields the most optimal results.

# Apriori (Market Basket Optimization)
- Goal is to generate a list of product sets and their corresponding level of relatedness to support one another.
- Have a list of products usually bought together.
- Ex. Pasta, french fries, soup, sour cream.

```python
  import pandas as pd
  dataset = pd.read_csv('Market_Basket_Optimisation.csv', header = None)

  transactions = []
  for i in range (0,7501):
      transactions.append([str(dataset.values[i,j] for j in range (0,20)]
  ```

  This will generate a list of related products with their correspoding levels of relatedness.

  OR, an easier way to do so is... using the pre-built Apriori package directly in python!

  ```python
from apyori import apriori

results = list(rules)
results_list = []
for i in range (0, len(results)):
    results_list.append(append('RULE:\t' + str(results[i][0]) + '\nSUPPORT:\t' + str(results[i][1]))
print (results_list)
```
### Results from running in Spyder (another DS package in Anaconda) 
<img width="659" height="470" alt="image" src="https://github.com/user-attachments/assets/cc262d67-ca61-4602-893d-d26abdba88f0" />

The higher the support, the higher the relatedness. Now we know which items will sell well together! 

Try not to run in Jupyter Notebook, it will produce something messy like this.
<img width="891" height="326" alt="image" src="https://github.com/user-attachments/assets/86fe27cc-0f65-460b-aac9-51e75abc7a5a" />


# Reinforcement Learning 
- Punishment and rewards for right and wrong

## Artificial Neural Networks
- TensorFlow
- Supervised Learning

(To be continued)
